model_name: tiny-llama-1.1b
logger_name: tensorboard
resume: false
devices: auto
seed: 42
data: TinyLlama
out_dir: out/pretrain/tiny-llama
tokenizer_dir: checkpoints/meta-llama/Llama-2-7b-hf
train:
  save_interval: 1000
  log_interval: 1
  global_batch_size: 512
  micro_batch_size: 4
  lr_warmup_steps: 2000
  epochs: null
  max_tokens: 3000000000000
  max_steps: null
  max_seq_length: null
  tie_embeddings: null
  learning_rate: 4e-4
  weight_decay: 1e-1
  beta1: 0.9
  beta2: 0.95
  max_norm: 1.0
  min_lr: 4.0e-05
eval:
  interval: 1000
  max_new_tokens: null
  max_iters: 100
